{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89c4f112",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_42371/4226923902.py:26: UserWarning: # 'dynamic_axes' is not recommended when dynamo=True, and may lead to 'torch._dynamo.exc.UserError: Constraints violated.' Supply the 'dynamic_shapes' argument instead if export is unsuccessful.\n",
      "  torch.onnx.export(\n",
      "W0207 13:52:07.387000 42371 torch/onnx/_internal/exporter/_compat.py:125] Setting ONNX exporter to use operator set version 18 because the requested opset_version 17 is a lower version than we have implementations for. Automatic version conversion will be performed, which may not be successful at converting to the requested version. If version conversion is unsuccessful, the opset version of the exported model will be kept at 18. Please consider setting opset_version >=18 to leverage latest ONNX features\n",
      "W0207 13:52:07.928000 42371 torch/onnx/_internal/exporter/_registration.py:110] torchvision is not installed. Skipping torchvision::nms\n",
      "W0207 13:52:07.930000 42371 torch/onnx/_internal/exporter/_registration.py:110] torchvision is not installed. Skipping torchvision::roi_align\n",
      "W0207 13:52:07.933000 42371 torch/onnx/_internal/exporter/_registration.py:110] torchvision is not installed. Skipping torchvision::roi_pool\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.onnx] Obtain model graph for `AudioEncoder([...]` with `torch.export.export(..., strict=False)`...\n",
      "[torch.onnx] Obtain model graph for `AudioEncoder([...]` with `torch.export.export(..., strict=False)`... ✅\n",
      "[torch.onnx] Run decomposition...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.12/copyreg.py:99: FutureWarning: `isinstance(treespec, LeafSpec)` is deprecated, use `isinstance(treespec, TreeSpec) and treespec.is_leaf()` instead.\n",
      "  return cls.__new__(cls, *args)\n",
      "The model version conversion is not supported by the onnxscript version converter and fallback is enabled. The model will be converted using the onnx C API (target version: 17).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.onnx] Run decomposition... ✅\n",
      "[torch.onnx] Translate the graph into ONNX...\n",
      "[torch.onnx] Translate the graph into ONNX... ✅\n",
      "Applied 16 of general pattern rewrite rules.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_42371/4226923902.py:39: UserWarning: Exporting a model while it is in training mode. Please ensure that this is intended, as it may lead to different behavior during inference. Calling model.eval() before export is recommended.\n",
      "  torch.onnx.export(\n",
      "/tmp/ipykernel_42371/4226923902.py:39: UserWarning: # 'dynamic_axes' is not recommended when dynamo=True, and may lead to 'torch._dynamo.exc.UserError: Constraints violated.' Supply the 'dynamic_shapes' argument instead if export is unsuccessful.\n",
      "  torch.onnx.export(\n",
      "W0207 13:52:13.737000 42371 torch/onnx/_internal/exporter/_compat.py:125] Setting ONNX exporter to use operator set version 18 because the requested opset_version 17 is a lower version than we have implementations for. Automatic version conversion will be performed, which may not be successful at converting to the requested version. If version conversion is unsuccessful, the opset version of the exported model will be kept at 18. Please consider setting opset_version >=18 to leverage latest ONNX features\n",
      "W0207 13:52:14.263000 42371 torch/onnx/_internal/exporter/_registration.py:110] torchvision is not installed. Skipping torchvision::nms\n",
      "W0207 13:52:14.265000 42371 torch/onnx/_internal/exporter/_registration.py:110] torchvision is not installed. Skipping torchvision::roi_align\n",
      "W0207 13:52:14.268000 42371 torch/onnx/_internal/exporter/_registration.py:110] torchvision is not installed. Skipping torchvision::roi_pool\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.onnx] Obtain model graph for `FunctionalDecoder([...]` with `torch.export.export(..., strict=False)`...\n",
      "[torch.onnx] Obtain model graph for `FunctionalDecoder([...]` with `torch.export.export(..., strict=False)`... ✅\n",
      "[torch.onnx] Run decomposition...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.12/copyreg.py:99: FutureWarning: `isinstance(treespec, LeafSpec)` is deprecated, use `isinstance(treespec, TreeSpec) and treespec.is_leaf()` instead.\n",
      "  return cls.__new__(cls, *args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.onnx] Run decomposition... ✅\n",
      "[torch.onnx] Translate the graph into ONNX...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model version conversion is not supported by the onnxscript version converter and fallback is enabled. The model will be converted using the onnx C API (target version: 17).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.onnx] Translate the graph into ONNX... ✅\n",
      "Applied 37 of general pattern rewrite rules.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "import whisper\n",
    "from whisper.model import MultiHeadAttention\n",
    "\n",
    "\n",
    "def main():\n",
    "    model = whisper.load_model(\"tiny.en\")\n",
    "    model.cpu().eval()\n",
    "    patch(model)\n",
    "\n",
    "    encoder = model.encoder.cpu()\n",
    "    decoder = FunctionalDecoder(model.decoder.cpu())\n",
    "\n",
    "    x_mel = torch.randn(1, 80, 3000)\n",
    "    x_tokens = torch.zeros((1, 10), dtype=torch.long).cpu()\n",
    "    x_audio = encoder(x_mel).cpu()\n",
    "\n",
    "    cache_self_attn = torch.zeros(\n",
    "        (len(decoder.keys_self_attn), 1, 1, model.dims.n_text_state),\n",
    "    )\n",
    "    cache_cross_attn = torch.zeros(\n",
    "        (len(decoder.keys_cross_attn), 1, 1, model.dims.n_audio_state),\n",
    "    )\n",
    "\n",
    "    torch.onnx.export(\n",
    "        encoder,\n",
    "        (x_mel,),\n",
    "        \"encoder.onnx\",\n",
    "        input_names=[\"mel\"],\n",
    "        output_names=[\"audio\"],\n",
    "        dynamic_axes={\n",
    "            \"mel\": {0: \"batch\", 2: \"time\"},\n",
    "            \"audio\": {0: \"batch\", 1: \"time\"},\n",
    "        },\n",
    "        opset_version=17,\n",
    "    )\n",
    "\n",
    "    torch.onnx.export(\n",
    "        decoder,\n",
    "        (x_tokens, x_audio, cache_self_attn, cache_cross_attn),\n",
    "        \"decoder.onnx\",\n",
    "        input_names=[\"tokens\", \"audio\", \"cache_self_attn\", \"cache_cross_attn\"],\n",
    "        output_names=[\"logits\", \"new_cache_self_attn\", \"new_cache_cross_attn\"],\n",
    "        dynamic_axes={\n",
    "            # inputs\n",
    "            \"tokens\": {0: \"batch\", 1: \"seq\"},\n",
    "            \"audio\": {0: \"batch\", 1: \"time\"},\n",
    "            \"cache_self_attn\": {2: \"cached_seq\"},\n",
    "            \"cache_cross_attn\": {2: \"cached_time\"},\n",
    "            # outputs\n",
    "            \"logits\": {0: \"batch\", 1: \"seq\"},\n",
    "            \"new_cache_self_attn\": {1: \"batch\", 2: \"new_cached_seq\"},\n",
    "            \"new_cache_cross_attn\": {1: \"batch\", 2: \"new_cached_time\"},\n",
    "        },\n",
    "        opset_version=17,\n",
    "    )\n",
    "\n",
    "\n",
    "def patch(model):\n",
    "    for block in model.decoder.blocks:\n",
    "        block.attn.__class__ = FunctionalMultiHeadAttention\n",
    "        block.attn.n_ctx = model.dims.n_text_ctx\n",
    "\n",
    "        block.cross_attn.__class__ = FunctionalMultiHeadAttention\n",
    "        block.cross_attn.n_ctx = model.dims.n_audio_ctx\n",
    "\n",
    "\n",
    "class FunctionalDecoder(torch.nn.Module):\n",
    "    def __init__(self, decoder):\n",
    "        super().__init__()\n",
    "        self.decoder = decoder\n",
    "\n",
    "        self.keys_self_attn = []\n",
    "        self.keys_cross_attn = []\n",
    "\n",
    "        for block in decoder.blocks:\n",
    "            self.keys_self_attn += (block.attn.key, block.attn.value)\n",
    "            self.keys_cross_attn += (block.cross_attn.key, block.cross_attn.value)\n",
    "\n",
    "    def forward(self, x, xa, cache_self_attn, cache_cross_attn):\n",
    "        kv_cache = {\n",
    "            **dict(zip(self.keys_self_attn, cache_self_attn)),\n",
    "            **dict(zip(self.keys_cross_attn, cache_cross_attn)),\n",
    "        }\n",
    "\n",
    "        logits = self.decoder(x, xa, kv_cache=kv_cache)\n",
    "        return (\n",
    "            logits,\n",
    "            torch.cat([kv_cache[key].unsqueeze(0) for key in self.keys_self_attn], dim=0),\n",
    "            torch.cat([kv_cache[key].unsqueeze(0) for key in self.keys_cross_attn], dim=0),\n",
    "        )\n",
    "\n",
    "\n",
    "class FunctionalMultiHeadAttention(MultiHeadAttention):\n",
    "    def forward(self, x, xa=None, mask=None, kv_cache=None):\n",
    "        k, v = self._get_kv(x, xa, kv_cache)\n",
    "\n",
    "        q = self.query(x)\n",
    "        wv, qk = self.qkv_attention(q, k, v, mask)\n",
    "        return self.out(wv), qk\n",
    "\n",
    "    def _get_kv(self, x, xa=None, kv_cache=None):\n",
    "        xx = x if xa is None else xa\n",
    "        assert xx is not None\n",
    "\n",
    "        if kv_cache is None:\n",
    "            return self.key(xx), self.value(xx)\n",
    "\n",
    "        key = torch.concat([kv_cache[self.key], self.key(xx).detach()], dim=1)\n",
    "        key = key[:, -self.n_ctx :, :]\n",
    "        kv_cache[self.key] = key\n",
    "\n",
    "        value = torch.concat([kv_cache[self.value], self.value(xx).detach()], dim=1)\n",
    "        value = value[:, -self.n_ctx :, :]\n",
    "        kv_cache[self.value] = value\n",
    "\n",
    "        return kv_cache[self.key], kv_cache[self.value]\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97ac6cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "android-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
