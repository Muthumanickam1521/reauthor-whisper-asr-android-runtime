{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69b9c91c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded: <|startoftranscript|><|notimestamps|>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import whisper\n",
    "import whisper.model\n",
    "from whisper.model import MultiHeadAttention\n",
    "\n",
    "# Disable SDPA (required)\n",
    "whisper.model.MultiHeadAttention.use_sdpa = False\n",
    "\n",
    "\n",
    "# ---------- Functional Self-Attention ----------\n",
    "class FunctionalSelfAttention(MultiHeadAttention):\n",
    "\n",
    "    def forward(self, x, xa=None, mask=None, kv_cache=None):\n",
    "\n",
    "        if kv_cache is None:\n",
    "            k = self.key(x)\n",
    "            v = self.value(x)\n",
    "        else:\n",
    "            k_prev = kv_cache[self.key]\n",
    "            v_prev = kv_cache[self.value]\n",
    "\n",
    "            k_new = self.key(x)\n",
    "            v_new = self.value(x)\n",
    "\n",
    "            k = torch.cat([k_prev, k_new], dim=1)\n",
    "            v = torch.cat([v_prev, v_new], dim=1)\n",
    "\n",
    "            kv_cache[self.key] = k\n",
    "            kv_cache[self.value] = v\n",
    "\n",
    "        q = self.query(x)\n",
    "        wv, qk = self.qkv_attention(q, k, v, mask)\n",
    "\n",
    "        return self.out(wv), qk\n",
    "\n",
    "\n",
    "# ---------- Patch model ----------\n",
    "def patch_model(model):\n",
    "    for block in model.decoder.blocks:\n",
    "        block.attn.__class__ = FunctionalSelfAttention\n",
    "\n",
    "\n",
    "# ---------- Functional Decoder ----------\n",
    "class FunctionalDecoder(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, decoder):\n",
    "        super().__init__()\n",
    "        self.decoder = decoder\n",
    "\n",
    "        self.cache_keys = []\n",
    "        for block in decoder.blocks:\n",
    "            self.cache_keys.append(block.attn.key)\n",
    "            self.cache_keys.append(block.attn.value)\n",
    "\n",
    "    def forward(self, tokens, audio, cache):\n",
    "\n",
    "        kv_cache = dict(zip(self.cache_keys, cache))\n",
    "\n",
    "        offset = kv_cache[self.cache_keys[0]].shape[1]\n",
    "\n",
    "        x = self.decoder.token_embedding(tokens)\n",
    "        x = x + self.decoder.positional_embedding[offset: offset + tokens.shape[1]]\n",
    "        x = x.to(audio.dtype)\n",
    "\n",
    "        mask = self.decoder.mask[\n",
    "            offset:offset + tokens.shape[1],\n",
    "            :offset + tokens.shape[1]\n",
    "        ]\n",
    "\n",
    "        for block in self.decoder.blocks:\n",
    "            x = block(x, audio, mask=mask, kv_cache=kv_cache)\n",
    "\n",
    "        x = self.decoder.ln(x)\n",
    "\n",
    "        logits = x @ self.decoder.token_embedding.weight.T\n",
    "\n",
    "        new_cache = torch.cat(\n",
    "            [kv_cache[k].unsqueeze(0) for k in self.cache_keys],\n",
    "            dim=0\n",
    "        )\n",
    "\n",
    "        return logits, new_cache\n",
    "\n",
    "\n",
    "\n",
    "# ---------- Main ----------\n",
    "def main():\n",
    "\n",
    "    model = whisper.load_model(\"tiny.en\").cpu().eval()\n",
    "    patch_model(model)\n",
    "\n",
    "    decoder = FunctionalDecoder(model.decoder)\n",
    "    tokenizer = whisper.tokenizer.get_tokenizer(True)\n",
    "\n",
    "    audio = whisper.load_audio(\"sample.mp3\")\n",
    "    audio = whisper.pad_or_trim(audio)\n",
    "\n",
    "    mel = whisper.log_mel_spectrogram(audio).unsqueeze(0)\n",
    "    audio_features = model.encoder(mel)\n",
    "\n",
    "    # CRITICAL: start with empty cache\n",
    "    cache = torch.zeros((8, 1, 0, 384))\n",
    "\n",
    "    # CRITICAL: correct start sequence\n",
    "    start_tokens = [tokenizer.sot, tokenizer.no_timestamps]\n",
    "\n",
    "    result = []\n",
    "\n",
    "    for t in start_tokens:\n",
    "        token_tensor = torch.tensor([[t]])\n",
    "        logits, cache = decoder(token_tensor, audio_features, cache)\n",
    "        result.append(t)\n",
    "\n",
    "    for _ in range(200):\n",
    "\n",
    "        next_token = logits[:, -1].argmax(-1).reshape(1, 1)\n",
    "        token_id = next_token.item()\n",
    "\n",
    "        if token_id == tokenizer.eot:\n",
    "            break\n",
    "\n",
    "        result.append(token_id)\n",
    "\n",
    "        logits, cache = decoder(next_token, audio_features, cache)\n",
    "\n",
    "    text = tokenizer.decode(result)\n",
    "\n",
    "    print(\"Decoded:\", text)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdece51a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9db5df4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c4f112",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import whisper\n",
    "from whisper.model import MultiHeadAttention\n",
    "\n",
    "\n",
    "def main():\n",
    "    model = whisper.load_model(\"tiny.en\")\n",
    "    model.cpu().eval()\n",
    "    patch(model)\n",
    "\n",
    "    encoder = model.encoder.cpu()\n",
    "    decoder = FunctionalDecoder(model.decoder.cpu())\n",
    "\n",
    "    x_mel = torch.randn(1, 80, 3000)\n",
    "    x_tokens = torch.zeros((1, 10), dtype=torch.long).cpu()\n",
    "    x_audio = encoder(x_mel).cpu()\n",
    "\n",
    "    cache_self_attn = torch.zeros(\n",
    "        (len(decoder.keys_self_attn), 1, 1, model.dims.n_text_state),\n",
    "    )\n",
    "    cache_cross_attn = torch.zeros(\n",
    "        (len(decoder.keys_cross_attn), 1, 1, model.dims.n_audio_state),\n",
    "    )\n",
    "\n",
    "    torch.onnx.export(\n",
    "        encoder,\n",
    "        (x_mel,),\n",
    "        \"encoder.onnx\",\n",
    "        input_names=[\"mel\"],\n",
    "        output_names=[\"audio\"],\n",
    "        dynamic_axes={\n",
    "            \"mel\": {0: \"batch\", 2: \"time\"},\n",
    "            \"audio\": {0: \"batch\", 1: \"time\"},\n",
    "        },\n",
    "        opset_version=17,\n",
    "    )\n",
    "\n",
    "    torch.onnx.export(\n",
    "        decoder,\n",
    "        (x_tokens, x_audio, cache_self_attn, cache_cross_attn),\n",
    "        \"decoder.onnx\",\n",
    "        input_names=[\"tokens\", \"audio\", \"cache_self_attn\", \"cache_cross_attn\"],\n",
    "        output_names=[\"logits\", \"new_cache_self_attn\", \"new_cache_cross_attn\"],\n",
    "        dynamic_axes={\n",
    "            # inputs\n",
    "            \"tokens\": {0: \"batch\", 1: \"seq\"},\n",
    "            \"audio\": {0: \"batch\", 1: \"time\"},\n",
    "            \"cache_self_attn\": {2: \"cached_seq\"},\n",
    "            \"cache_cross_attn\": {2: \"cached_time\"},\n",
    "            # outputs\n",
    "            \"logits\": {0: \"batch\", 1: \"seq\"},\n",
    "            \"new_cache_self_attn\": {1: \"batch\", 2: \"new_cached_seq\"},\n",
    "            \"new_cache_cross_attn\": {1: \"batch\", 2: \"new_cached_time\"},\n",
    "        },\n",
    "        opset_version=17,\n",
    "    )\n",
    "\n",
    "\n",
    "def patch(model):\n",
    "    for block in model.decoder.blocks:\n",
    "        block.attn.__class__ = FunctionalMultiHeadAttention\n",
    "        block.attn.n_ctx = model.dims.n_text_ctx\n",
    "\n",
    "        block.cross_attn.__class__ = FunctionalMultiHeadAttention\n",
    "        block.cross_attn.n_ctx = model.dims.n_audio_ctx\n",
    "\n",
    "\n",
    "class FunctionalDecoder(torch.nn.Module):\n",
    "    def __init__(self, decoder):\n",
    "        super().__init__()\n",
    "        self.decoder = decoder\n",
    "\n",
    "        self.keys_self_attn = []\n",
    "        self.keys_cross_attn = []\n",
    "\n",
    "        for block in decoder.blocks:\n",
    "            self.keys_self_attn += (block.attn.key, block.attn.value)\n",
    "            self.keys_cross_attn += (block.cross_attn.key, block.cross_attn.value)\n",
    "\n",
    "    def forward(self, x, xa, cache_self_attn, cache_cross_attn):\n",
    "        kv_cache = {\n",
    "            **dict(zip(self.keys_self_attn, cache_self_attn)),\n",
    "            **dict(zip(self.keys_cross_attn, cache_cross_attn)),\n",
    "        }\n",
    "\n",
    "        logits = self.decoder(x, xa, kv_cache=kv_cache)\n",
    "        return (\n",
    "            logits,\n",
    "            torch.cat([kv_cache[key].unsqueeze(0) for key in self.keys_self_attn], dim=0),\n",
    "            torch.cat([kv_cache[key].unsqueeze(0) for key in self.keys_cross_attn], dim=0),\n",
    "        )\n",
    "\n",
    "\n",
    "class FunctionalMultiHeadAttention(MultiHeadAttention):\n",
    "    def forward(self, x, xa=None, mask=None, kv_cache=None):\n",
    "        k, v = self._get_kv(x, xa, kv_cache)\n",
    "\n",
    "        q = self.query(x)\n",
    "        wv, qk = self.qkv_attention(q, k, v, mask)\n",
    "        return self.out(wv), qk\n",
    "\n",
    "    def _get_kv(self, x, xa=None, kv_cache=None):\n",
    "        xx = x if xa is None else xa\n",
    "        assert xx is not None\n",
    "\n",
    "        if kv_cache is None:\n",
    "            return self.key(xx), self.value(xx)\n",
    "\n",
    "        key = torch.concat([kv_cache[self.key], self.key(xx).detach()], dim=1)\n",
    "        key = key[:, -self.n_ctx :, :]\n",
    "        kv_cache[self.key] = key\n",
    "\n",
    "        value = torch.concat([kv_cache[self.value], self.value(xx).detach()], dim=1)\n",
    "        value = value[:, -self.n_ctx :, :]\n",
    "        kv_cache[self.value] = value\n",
    "\n",
    "        return kv_cache[self.key], kv_cache[self.value]\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd8740c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ab319717",
   "metadata": {},
   "source": [
    "corrected enhanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a769c90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430c34dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gtts import gTTS\n",
    "\n",
    "text = \"Hello, this is a Whisper ONNX test.\"\n",
    "tts = gTTS(text=text, lang=\"en\")\n",
    "\n",
    "tts.save(\"sample.mp3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc51b3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "import whisper.model\n",
    "whisper.model.MultiHeadAttention.use_sdpa = False\n",
    "\n",
    "model = whisper.load_model(\"tiny\")\n",
    "\n",
    "result = model.transcribe(\"sample.mp3\")\n",
    "\n",
    "\n",
    "print(result[\"text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefd8789",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import whisper\n",
    "import onnxruntime as ort\n",
    "\n",
    "# tokenizer\n",
    "tokenizer = whisper.tokenizer.get_tokenizer(True)\n",
    "\n",
    "# load audio â†’ mel\n",
    "audio = whisper.load_audio(\"sample.mp3\")\n",
    "audio = whisper.pad_or_trim(audio)\n",
    "mel = whisper.log_mel_spectrogram(audio).unsqueeze(0)\n",
    "mel = mel.cpu().numpy().astype(np.float32)\n",
    "\n",
    "# load ONNX\n",
    "enc = ort.InferenceSession(\"encoder.onnx\")\n",
    "dec = ort.InferenceSession(\"decoder.onnx\")\n",
    "\n",
    "# encoder forward\n",
    "audio_features = enc.run(None, {\"mel\": mel})[0].astype(np.float32)\n",
    "\n",
    "# KV cache init\n",
    "cache_self = np.zeros((8,1,1,384), np.float32)\n",
    "cache_cross = np.zeros((8,1,1,384), np.float32)\n",
    "\n",
    "# PREFILL cross-attn cache (CRITICAL)\n",
    "dummy = np.array([[tokenizer.sot]], dtype=np.int64)\n",
    "_, _, cache_cross = dec.run(\n",
    "    None,\n",
    "    {\n",
    "        \"tokens\": dummy,\n",
    "        \"audio\": audio_features,\n",
    "        \"cache_self_attn\": cache_self,\n",
    "        \"cache_cross_attn\": cache_cross,\n",
    "    },\n",
    ")\n",
    "\n",
    "# reset self cache only\n",
    "cache_self[:] = 0\n",
    "\n",
    "# start decoding\n",
    "tokens_list = [tokenizer.sot]\n",
    "tokens = np.array([[tokenizer.sot]], dtype=np.int64)\n",
    "\n",
    "logits, cache_self, cache_cross = dec.run(\n",
    "    None,\n",
    "    {\n",
    "        \"tokens\": tokens,\n",
    "        \"audio\": audio_features,\n",
    "        \"cache_self_attn\": cache_self,\n",
    "        \"cache_cross_attn\": cache_cross,\n",
    "    },\n",
    ")\n",
    "\n",
    "# incremental decode\n",
    "for _ in range(200):\n",
    "    next_token = logits[:, -1, :].argmax(-1).astype(np.int64).reshape(1,1)\n",
    "\n",
    "    if next_token[0,0] == tokenizer.eot:\n",
    "        break\n",
    "\n",
    "    tokens_list.append(int(next_token[0,0]))\n",
    "\n",
    "    logits, cache_self, cache_cross = dec.run(\n",
    "        None,\n",
    "        {\n",
    "            \"tokens\": next_token,\n",
    "            \"audio\": audio_features,\n",
    "            \"cache_self_attn\": cache_self,\n",
    "            \"cache_cross_attn\": cache_cross,\n",
    "        },\n",
    "    )\n",
    "\n",
    "# decode text\n",
    "print(tokenizer.decode(tokens_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90feb03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "android-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
