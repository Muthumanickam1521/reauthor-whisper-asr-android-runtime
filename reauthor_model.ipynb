{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89c4f112",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_42371/4226923902.py:26: UserWarning: # 'dynamic_axes' is not recommended when dynamo=True, and may lead to 'torch._dynamo.exc.UserError: Constraints violated.' Supply the 'dynamic_shapes' argument instead if export is unsuccessful.\n",
      "  torch.onnx.export(\n",
      "W0207 13:52:07.387000 42371 torch/onnx/_internal/exporter/_compat.py:125] Setting ONNX exporter to use operator set version 18 because the requested opset_version 17 is a lower version than we have implementations for. Automatic version conversion will be performed, which may not be successful at converting to the requested version. If version conversion is unsuccessful, the opset version of the exported model will be kept at 18. Please consider setting opset_version >=18 to leverage latest ONNX features\n",
      "W0207 13:52:07.928000 42371 torch/onnx/_internal/exporter/_registration.py:110] torchvision is not installed. Skipping torchvision::nms\n",
      "W0207 13:52:07.930000 42371 torch/onnx/_internal/exporter/_registration.py:110] torchvision is not installed. Skipping torchvision::roi_align\n",
      "W0207 13:52:07.933000 42371 torch/onnx/_internal/exporter/_registration.py:110] torchvision is not installed. Skipping torchvision::roi_pool\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.onnx] Obtain model graph for `AudioEncoder([...]` with `torch.export.export(..., strict=False)`...\n",
      "[torch.onnx] Obtain model graph for `AudioEncoder([...]` with `torch.export.export(..., strict=False)`... ✅\n",
      "[torch.onnx] Run decomposition...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.12/copyreg.py:99: FutureWarning: `isinstance(treespec, LeafSpec)` is deprecated, use `isinstance(treespec, TreeSpec) and treespec.is_leaf()` instead.\n",
      "  return cls.__new__(cls, *args)\n",
      "The model version conversion is not supported by the onnxscript version converter and fallback is enabled. The model will be converted using the onnx C API (target version: 17).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.onnx] Run decomposition... ✅\n",
      "[torch.onnx] Translate the graph into ONNX...\n",
      "[torch.onnx] Translate the graph into ONNX... ✅\n",
      "Applied 16 of general pattern rewrite rules.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_42371/4226923902.py:39: UserWarning: Exporting a model while it is in training mode. Please ensure that this is intended, as it may lead to different behavior during inference. Calling model.eval() before export is recommended.\n",
      "  torch.onnx.export(\n",
      "/tmp/ipykernel_42371/4226923902.py:39: UserWarning: # 'dynamic_axes' is not recommended when dynamo=True, and may lead to 'torch._dynamo.exc.UserError: Constraints violated.' Supply the 'dynamic_shapes' argument instead if export is unsuccessful.\n",
      "  torch.onnx.export(\n",
      "W0207 13:52:13.737000 42371 torch/onnx/_internal/exporter/_compat.py:125] Setting ONNX exporter to use operator set version 18 because the requested opset_version 17 is a lower version than we have implementations for. Automatic version conversion will be performed, which may not be successful at converting to the requested version. If version conversion is unsuccessful, the opset version of the exported model will be kept at 18. Please consider setting opset_version >=18 to leverage latest ONNX features\n",
      "W0207 13:52:14.263000 42371 torch/onnx/_internal/exporter/_registration.py:110] torchvision is not installed. Skipping torchvision::nms\n",
      "W0207 13:52:14.265000 42371 torch/onnx/_internal/exporter/_registration.py:110] torchvision is not installed. Skipping torchvision::roi_align\n",
      "W0207 13:52:14.268000 42371 torch/onnx/_internal/exporter/_registration.py:110] torchvision is not installed. Skipping torchvision::roi_pool\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.onnx] Obtain model graph for `FunctionalDecoder([...]` with `torch.export.export(..., strict=False)`...\n",
      "[torch.onnx] Obtain model graph for `FunctionalDecoder([...]` with `torch.export.export(..., strict=False)`... ✅\n",
      "[torch.onnx] Run decomposition...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.12/copyreg.py:99: FutureWarning: `isinstance(treespec, LeafSpec)` is deprecated, use `isinstance(treespec, TreeSpec) and treespec.is_leaf()` instead.\n",
      "  return cls.__new__(cls, *args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.onnx] Run decomposition... ✅\n",
      "[torch.onnx] Translate the graph into ONNX...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model version conversion is not supported by the onnxscript version converter and fallback is enabled. The model will be converted using the onnx C API (target version: 17).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.onnx] Translate the graph into ONNX... ✅\n",
      "Applied 37 of general pattern rewrite rules.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "import whisper\n",
    "from whisper.model import MultiHeadAttention\n",
    "\n",
    "\n",
    "def main():\n",
    "    model = whisper.load_model(\"tiny.en\")\n",
    "    model.cpu().eval()\n",
    "    patch(model)\n",
    "\n",
    "    encoder = model.encoder.cpu()\n",
    "    decoder = FunctionalDecoder(model.decoder.cpu())\n",
    "\n",
    "    x_mel = torch.randn(1, 80, 3000)\n",
    "    x_tokens = torch.zeros((1, 10), dtype=torch.long).cpu()\n",
    "    x_audio = encoder(x_mel).cpu()\n",
    "\n",
    "    cache_self_attn = torch.zeros(\n",
    "        (len(decoder.keys_self_attn), 1, 1, model.dims.n_text_state),\n",
    "    )\n",
    "    cache_cross_attn = torch.zeros(\n",
    "        (len(decoder.keys_cross_attn), 1, 1, model.dims.n_audio_state),\n",
    "    )\n",
    "\n",
    "    torch.onnx.export(\n",
    "        encoder,\n",
    "        (x_mel,),\n",
    "        \"encoder.onnx\",\n",
    "        input_names=[\"mel\"],\n",
    "        output_names=[\"audio\"],\n",
    "        dynamic_axes={\n",
    "            \"mel\": {0: \"batch\", 2: \"time\"},\n",
    "            \"audio\": {0: \"batch\", 1: \"time\"},\n",
    "        },\n",
    "        opset_version=17,\n",
    "    )\n",
    "\n",
    "    torch.onnx.export(\n",
    "        decoder,\n",
    "        (x_tokens, x_audio, cache_self_attn, cache_cross_attn),\n",
    "        \"decoder.onnx\",\n",
    "        input_names=[\"tokens\", \"audio\", \"cache_self_attn\", \"cache_cross_attn\"],\n",
    "        output_names=[\"logits\", \"new_cache_self_attn\", \"new_cache_cross_attn\"],\n",
    "        dynamic_axes={\n",
    "            # inputs\n",
    "            \"tokens\": {0: \"batch\", 1: \"seq\"},\n",
    "            \"audio\": {0: \"batch\", 1: \"time\"},\n",
    "            \"cache_self_attn\": {2: \"cached_seq\"},\n",
    "            \"cache_cross_attn\": {2: \"cached_time\"},\n",
    "            # outputs\n",
    "            \"logits\": {0: \"batch\", 1: \"seq\"},\n",
    "            \"new_cache_self_attn\": {1: \"batch\", 2: \"new_cached_seq\"},\n",
    "            \"new_cache_cross_attn\": {1: \"batch\", 2: \"new_cached_time\"},\n",
    "        },\n",
    "        opset_version=17,\n",
    "    )\n",
    "\n",
    "\n",
    "def patch(model):\n",
    "    for block in model.decoder.blocks:\n",
    "        block.attn.__class__ = FunctionalMultiHeadAttention\n",
    "        block.attn.n_ctx = model.dims.n_text_ctx\n",
    "\n",
    "        block.cross_attn.__class__ = FunctionalMultiHeadAttention\n",
    "        block.cross_attn.n_ctx = model.dims.n_audio_ctx\n",
    "\n",
    "\n",
    "class FunctionalDecoder(torch.nn.Module):\n",
    "    def __init__(self, decoder):\n",
    "        super().__init__()\n",
    "        self.decoder = decoder\n",
    "\n",
    "        self.keys_self_attn = []\n",
    "        self.keys_cross_attn = []\n",
    "\n",
    "        for block in decoder.blocks:\n",
    "            self.keys_self_attn += (block.attn.key, block.attn.value)\n",
    "            self.keys_cross_attn += (block.cross_attn.key, block.cross_attn.value)\n",
    "\n",
    "    def forward(self, x, xa, cache_self_attn, cache_cross_attn):\n",
    "        kv_cache = {\n",
    "            **dict(zip(self.keys_self_attn, cache_self_attn)),\n",
    "            **dict(zip(self.keys_cross_attn, cache_cross_attn)),\n",
    "        }\n",
    "\n",
    "        logits = self.decoder(x, xa, kv_cache=kv_cache)\n",
    "        return (\n",
    "            logits,\n",
    "            torch.cat([kv_cache[key].unsqueeze(0) for key in self.keys_self_attn], dim=0),\n",
    "            torch.cat([kv_cache[key].unsqueeze(0) for key in self.keys_cross_attn], dim=0),\n",
    "        )\n",
    "\n",
    "\n",
    "class FunctionalMultiHeadAttention(MultiHeadAttention):\n",
    "    def forward(self, x, xa=None, mask=None, kv_cache=None):\n",
    "        k, v = self._get_kv(x, xa, kv_cache)\n",
    "\n",
    "        q = self.query(x)\n",
    "        wv, qk = self.qkv_attention(q, k, v, mask)\n",
    "        return self.out(wv), qk\n",
    "\n",
    "    def _get_kv(self, x, xa=None, kv_cache=None):\n",
    "        xx = x if xa is None else xa\n",
    "        assert xx is not None\n",
    "\n",
    "        if kv_cache is None:\n",
    "            return self.key(xx), self.value(xx)\n",
    "\n",
    "        key = torch.concat([kv_cache[self.key], self.key(xx).detach()], dim=1)\n",
    "        key = key[:, -self.n_ctx :, :]\n",
    "        kv_cache[self.key] = key\n",
    "\n",
    "        value = torch.concat([kv_cache[self.value], self.value(xx).detach()], dim=1)\n",
    "        value = value[:, -self.n_ctx :, :]\n",
    "        kv_cache[self.value] = value\n",
    "\n",
    "        return kv_cache[self.key], kv_cache[self.value]\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd8740c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ab319717",
   "metadata": {},
   "source": [
    "corrected enhanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a769c90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_53297/359843868.py:36: UserWarning: # 'dynamic_axes' is not recommended when dynamo=True, and may lead to 'torch._dynamo.exc.UserError: Constraints violated.' Supply the 'dynamic_shapes' argument instead if export is unsuccessful.\n",
      "  torch.onnx.export(\n",
      "W0207 18:56:00.714000 53297 torch/onnx/_internal/exporter/_compat.py:125] Setting ONNX exporter to use operator set version 18 because the requested opset_version 17 is a lower version than we have implementations for. Automatic version conversion will be performed, which may not be successful at converting to the requested version. If version conversion is unsuccessful, the opset version of the exported model will be kept at 18. Please consider setting opset_version >=18 to leverage latest ONNX features\n",
      "W0207 18:56:01.292000 53297 torch/onnx/_internal/exporter/_registration.py:110] torchvision is not installed. Skipping torchvision::nms\n",
      "W0207 18:56:01.294000 53297 torch/onnx/_internal/exporter/_registration.py:110] torchvision is not installed. Skipping torchvision::roi_align\n",
      "W0207 18:56:01.296000 53297 torch/onnx/_internal/exporter/_registration.py:110] torchvision is not installed. Skipping torchvision::roi_pool\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.onnx] Obtain model graph for `AudioEncoder([...]` with `torch.export.export(..., strict=False)`...\n",
      "[torch.onnx] Obtain model graph for `AudioEncoder([...]` with `torch.export.export(..., strict=False)`... ✅\n",
      "[torch.onnx] Run decomposition...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.12/copyreg.py:99: FutureWarning: `isinstance(treespec, LeafSpec)` is deprecated, use `isinstance(treespec, TreeSpec) and treespec.is_leaf()` instead.\n",
      "  return cls.__new__(cls, *args)\n",
      "The model version conversion is not supported by the onnxscript version converter and fallback is enabled. The model will be converted using the onnx C API (target version: 17).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.onnx] Run decomposition... ✅\n",
      "[torch.onnx] Translate the graph into ONNX...\n",
      "[torch.onnx] Translate the graph into ONNX... ✅\n",
      "Applied 16 of general pattern rewrite rules.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_53297/359843868.py:50: UserWarning: Exporting a model while it is in training mode. Please ensure that this is intended, as it may lead to different behavior during inference. Calling model.eval() before export is recommended.\n",
      "  torch.onnx.export(\n",
      "/tmp/ipykernel_53297/359843868.py:50: UserWarning: # 'dynamic_axes' is not recommended when dynamo=True, and may lead to 'torch._dynamo.exc.UserError: Constraints violated.' Supply the 'dynamic_shapes' argument instead if export is unsuccessful.\n",
      "  torch.onnx.export(\n",
      "W0207 18:56:06.836000 53297 torch/onnx/_internal/exporter/_compat.py:125] Setting ONNX exporter to use operator set version 18 because the requested opset_version 17 is a lower version than we have implementations for. Automatic version conversion will be performed, which may not be successful at converting to the requested version. If version conversion is unsuccessful, the opset version of the exported model will be kept at 18. Please consider setting opset_version >=18 to leverage latest ONNX features\n",
      "W0207 18:56:07.428000 53297 torch/onnx/_internal/exporter/_registration.py:110] torchvision is not installed. Skipping torchvision::nms\n",
      "W0207 18:56:07.430000 53297 torch/onnx/_internal/exporter/_registration.py:110] torchvision is not installed. Skipping torchvision::roi_align\n",
      "W0207 18:56:07.433000 53297 torch/onnx/_internal/exporter/_registration.py:110] torchvision is not installed. Skipping torchvision::roi_pool\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.onnx] Obtain model graph for `FunctionalDecoder([...]` with `torch.export.export(..., strict=False)`...\n",
      "[torch.onnx] Obtain model graph for `FunctionalDecoder([...]` with `torch.export.export(..., strict=False)`... ✅\n",
      "[torch.onnx] Run decomposition...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.12/copyreg.py:99: FutureWarning: `isinstance(treespec, LeafSpec)` is deprecated, use `isinstance(treespec, TreeSpec) and treespec.is_leaf()` instead.\n",
      "  return cls.__new__(cls, *args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.onnx] Run decomposition... ✅\n",
      "[torch.onnx] Translate the graph into ONNX...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pearlrubymv/reauthor-whisper-asr-android-runtime/android-dev/lib/python3.12/site-packages/torch/onnx/_internal/exporter/_onnx_program.py:460: UserWarning: # The axis name: batch will not be used, since it shares the same shape constraints with another axis: batch.\n",
      "  rename_mapping = _dynamic_shapes.create_rename_mapping(\n",
      "The model version conversion is not supported by the onnxscript version converter and fallback is enabled. The model will be converted using the onnx C API (target version: 17).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.onnx] Translate the graph into ONNX... ✅\n",
      "Applied 47 of general pattern rewrite rules.\n",
      "Export complete: encoder.onnx, decoder.onnx\n"
     ]
    }
   ],
   "source": [
    "# whisper_to_onnx.py\n",
    "# Production-grade export with external KV cache (Android / ONNX Runtime)\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import torch\n",
    "import whisper\n",
    "from whisper.model import MultiHeadAttention\n",
    "\n",
    "\n",
    "OPSET = 17\n",
    "\n",
    "\n",
    "def main():\n",
    "    model = whisper.load_model(\"tiny.en\").cpu().eval()\n",
    "    _patch_decoder(model)\n",
    "\n",
    "    encoder = model.encoder\n",
    "    decoder = FunctionalDecoder(model.decoder)\n",
    "\n",
    "    # dummy inputs (non-zero cache length required)\n",
    "    mel = torch.randn(1, 80, 3000, dtype=torch.float32)\n",
    "    audio = encoder(mel)\n",
    "\n",
    "    n_text = model.dims.n_text_state\n",
    "    n_audio = model.dims.n_audio_state\n",
    "    n_self = len(decoder.keys_self_attn)\n",
    "    n_cross = len(decoder.keys_cross_attn)\n",
    "\n",
    "    cache_self = torch.zeros((n_self, 1, 1, n_text), dtype=torch.float32)\n",
    "    cache_cross = torch.zeros((n_cross, 1, 1, n_audio), dtype=torch.float32)\n",
    "\n",
    "    tokens = torch.zeros((1, 1), dtype=torch.long)\n",
    "\n",
    "    # export encoder\n",
    "    torch.onnx.export(\n",
    "        encoder,\n",
    "        (mel,),\n",
    "        \"encoder.onnx\",\n",
    "        input_names=[\"mel\"],\n",
    "        output_names=[\"audio\"],\n",
    "        dynamic_axes={\n",
    "            \"mel\": {0: \"batch\", 2: \"time\"},\n",
    "            \"audio\": {0: \"batch\", 1: \"time\"},\n",
    "        },\n",
    "        opset_version=OPSET,\n",
    "    )\n",
    "\n",
    "    # export decoder\n",
    "    torch.onnx.export(\n",
    "        decoder,\n",
    "        (tokens, audio, cache_self, cache_cross),\n",
    "        \"decoder.onnx\",\n",
    "        input_names=[\n",
    "            \"tokens\",\n",
    "            \"audio\",\n",
    "            \"cache_self_attn\",\n",
    "            \"cache_cross_attn\",\n",
    "        ],\n",
    "        output_names=[\n",
    "            \"logits\",\n",
    "            \"new_cache_self_attn\",\n",
    "            \"new_cache_cross_attn\",\n",
    "        ],\n",
    "        dynamic_axes={\n",
    "            \"tokens\": {0: \"batch\", 1: \"seq\"},\n",
    "            \"audio\": {0: \"batch\", 1: \"time\"},\n",
    "            \"cache_self_attn\": {2: \"cached_seq\"},\n",
    "            \"cache_cross_attn\": {2: \"cached_time\"},\n",
    "            \"new_cache_self_attn\": {2: \"cached_seq\"},\n",
    "            \"new_cache_cross_attn\": {2: \"cached_time\"},\n",
    "        },\n",
    "        opset_version=OPSET,\n",
    "    )\n",
    "\n",
    "    print(\"Export complete: encoder.onnx, decoder.onnx\")\n",
    "\n",
    "\n",
    "def _patch_decoder(model):\n",
    "    \"\"\"Replace attention with functional KV-cache aware version.\"\"\"\n",
    "    for block in model.decoder.blocks:\n",
    "        block.attn.__class__ = FunctionalMultiHeadAttention\n",
    "        block.cross_attn.__class__ = FunctionalMultiHeadAttention\n",
    "\n",
    "        block.attn.n_ctx = model.dims.n_text_ctx\n",
    "        block.cross_attn.n_ctx = model.dims.n_audio_ctx\n",
    "\n",
    "\n",
    "class FunctionalDecoder(torch.nn.Module):\n",
    "    \"\"\"Decoder wrapper exposing KV cache as tensors.\"\"\"\n",
    "\n",
    "    def __init__(self, decoder):\n",
    "        super().__init__()\n",
    "        self.decoder = decoder\n",
    "\n",
    "        self.keys_self_attn = []\n",
    "        self.keys_cross_attn = []\n",
    "\n",
    "        for block in decoder.blocks:\n",
    "            self.keys_self_attn += (block.attn.key, block.attn.value)\n",
    "            self.keys_cross_attn += (block.cross_attn.key, block.cross_attn.value)\n",
    "\n",
    "    def forward(self, tokens, audio, cache_self, cache_cross):\n",
    "        kv_cache = {\n",
    "            **dict(zip(self.keys_self_attn, cache_self)),\n",
    "            **dict(zip(self.keys_cross_attn, cache_cross)),\n",
    "        }\n",
    "\n",
    "        logits = self.decoder(tokens, audio, kv_cache=kv_cache)\n",
    "\n",
    "        new_self = torch.cat(\n",
    "            [kv_cache[k].unsqueeze(0) for k in self.keys_self_attn], dim=0\n",
    "        )\n",
    "        new_cross = torch.cat(\n",
    "            [kv_cache[k].unsqueeze(0) for k in self.keys_cross_attn], dim=0\n",
    "        )\n",
    "\n",
    "        return logits, new_self, new_cross\n",
    "\n",
    "\n",
    "class FunctionalMultiHeadAttention(MultiHeadAttention):\n",
    "    \"\"\"Attention with external KV cache support.\"\"\"\n",
    "\n",
    "    def forward(self, x, xa=None, mask=None, kv_cache=None):\n",
    "        k, v = self._get_kv(x, xa, kv_cache)\n",
    "        q = self.query(x)\n",
    "        wv, qk = self.qkv_attention(q, k, v, mask)\n",
    "        return self.out(wv), qk\n",
    "        \n",
    "\n",
    "    def _get_kv(self, x, xa, kv_cache):\n",
    "        if kv_cache is None:\n",
    "            src = xa if xa is not None else x\n",
    "            return self.key(src), self.value(src)\n",
    "\n",
    "        # cross-attention: compute once\n",
    "        if xa is not None:\n",
    "            if kv_cache[self.key].shape[1] <= 1:\n",
    "                kv_cache[self.key] = self.key(xa).detach()\n",
    "                kv_cache[self.value] = self.value(xa).detach()\n",
    "            return kv_cache[self.key], kv_cache[self.value]\n",
    "\n",
    "        # self-attention: incremental append\n",
    "        key = torch.cat([kv_cache[self.key], self.key(x).detach()], dim=1)\n",
    "        value = torch.cat([kv_cache[self.value], self.value(x).detach()], dim=1)\n",
    "\n",
    "        kv_cache[self.key] = key[:, -self.n_ctx :, :]\n",
    "        kv_cache[self.value] = value[:, -self.n_ctx :, :]\n",
    "\n",
    "        return kv_cache[self.key], kv_cache[self.value]\n",
    "\n",
    "\n",
    "def clear_existing_exports():\n",
    "    for filename in [\"encoder.onnx\", \"encoder.onnx.data\", \"decoder.onnx\", \"decoder.onnx.data\"]:\n",
    "        if os.path.exists(filename):\n",
    "            os.remove(filename)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    clear_existing_exports()\n",
    "\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "430c34dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gtts import gTTS\n",
    "\n",
    "text = \"Hello, this is a Whisper ONNX test.\"\n",
    "tts = gTTS(text=text, lang=\"en\")\n",
    "\n",
    "tts.save(\"sample.mp3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc51b3b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hello, this is a whisper ONNX test.\n"
     ]
    }
   ],
   "source": [
    "import whisper\n",
    "import whisper.model\n",
    "whisper.model.MultiHeadAttention.use_sdpa = False\n",
    "\n",
    "model = whisper.load_model(\"tiny\")\n",
    "\n",
    "result = model.transcribe(\"sample.mp3\")\n",
    "\n",
    "\n",
    "print(result[\"text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cefd8789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|startoftranscript|><|notimestamps|>atatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatat\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import whisper\n",
    "import onnxruntime as ort\n",
    "\n",
    "# load tokenizer\n",
    "tokenizer = whisper.tokenizer.get_tokenizer(True)\n",
    "\n",
    "# load audio\n",
    "audio = whisper.load_audio(\"sample.wav\")\n",
    "audio = whisper.pad_or_trim(audio)\n",
    "mel = whisper.log_mel_spectrogram(audio).unsqueeze(0).numpy()\n",
    "\n",
    "# load ONNX sessions\n",
    "enc = ort.InferenceSession(\"encoder.onnx\")\n",
    "dec = ort.InferenceSession(\"decoder.onnx\")\n",
    "\n",
    "# run encoder\n",
    "audio_features = enc.run(None, {\"mel\": mel})[0]\n",
    "\n",
    "# init KV cache (tiny.en = 4 layers × key,value = 8)\n",
    "cache_self = np.zeros((8, 1, 1, 384), dtype=np.float32)\n",
    "cache_cross = np.zeros((8, 1, 1, 384), dtype=np.float32)\n",
    "\n",
    "# correct start tokens\n",
    "tokens = [[tokenizer.sot, tokenizer.no_timestamps]]\n",
    "\n",
    "# first decoder run\n",
    "logits, cache_self, cache_cross = dec.run(\n",
    "    None,\n",
    "    {\n",
    "        \"tokens\": np.array(tokens, dtype=np.int64),\n",
    "        \"audio\": audio_features,\n",
    "        \"cache_self_attn\": cache_self,\n",
    "        \"cache_cross_attn\": cache_cross,\n",
    "    },\n",
    ")\n",
    "\n",
    "# incremental decoding\n",
    "for _ in range(100):\n",
    "    next_token = logits[:, -1, :].argmax(axis=-1).astype(np.int64).reshape(1,1)\n",
    "\n",
    "    if next_token[0,0] == tokenizer.eot:\n",
    "        break\n",
    "\n",
    "    tokens[0].append(int(next_token[0,0]))\n",
    "\n",
    "    logits, cache_self, cache_cross = dec.run(\n",
    "        None,\n",
    "        {\n",
    "            \"tokens\": next_token,\n",
    "            \"audio\": audio_features,\n",
    "            \"cache_self_attn\": cache_self,\n",
    "            \"cache_cross_attn\": cache_cross,\n",
    "        },\n",
    "    )\n",
    "\n",
    "# decode text\n",
    "text = tokenizer.decode(tokens[0])\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65395c5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "android-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
